{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Feature_Extraction_For_Testdata.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMHUSKZvYlYN"
      },
      "source": [
        "# Importing libraries\n",
        "\n",
        "import language_check\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys, re, os, nltk\n",
        "import requests\n",
        "import warnings\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import words, wordnet\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tree import Tree\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from textstat.textstat import textstat\n",
        "\n",
        "nlp = StanfordCoreNLP(r'C:\\Users\\hp word\\anaconda3\\Lib\\site-packages\\stanfordcorenlp')\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM0DhCvbZfzS"
      },
      "source": [
        "# Common functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6UDr-cNYlYT"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "output_list = []\n",
        "wordset = set(words.words())\n",
        "lmtzr = WordNetLemmatizer()\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "tool = language_check.LanguageTool('en-US')\n",
        "\n",
        "def restrict2TwoDecimals(D):\n",
        "    return round(D, 2)\n",
        "\n",
        "def handleDivByZero(n_1, n_2):\n",
        "    if n_2 == 0.0:\n",
        "        return 0.0;\n",
        "    elif n_1/n_2 > 10000:\n",
        "        return 0.0;\n",
        "    else:\n",
        "        return restrict2TwoDecimals(n_1/n_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2pxhPPiZlw_"
      },
      "source": [
        "# Reading output of previous txt to csv converter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt5d4oV1YlYV"
      },
      "source": [
        "testing_data = pd.read_csv('test_data.csv' ,encoding='ANSI')\n",
        "test_text = testing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LuGJGZYlYY"
      },
      "source": [
        "# Readability Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWAAarPiYlYc"
      },
      "source": [
        "# Using textstat library implementation of readability features\n",
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text'] \n",
        "    fre = textstat.flesch_reading_ease(essay)\n",
        "    fkg = textstat.flesch_kincaid_grade(essay)\n",
        "    cli = textstat.coleman_liau_index(essay)\n",
        "    ari = textstat.automated_readability_index(essay)\n",
        "    dcrs = textstat.dale_chall_readability_score(essay)\n",
        "    dw = textstat.difficult_words(essay)\n",
        "    lwf = textstat.linsear_write_formula(essay)\n",
        "    gf = textstat.gunning_fog(essay)\n",
        "    test_text.at[index, 'fre'] = fre\n",
        "    test_text.at[index, 'fkg'] = fkg\n",
        "    test_text.at[index, 'cli'] = cli\n",
        "    test_text.at[index, 'ari'] = ari\n",
        "    test_text.at[index, 'dcrs'] = dcrs\n",
        "    test_text.at[index, 'dw'] = dw\n",
        "    test_text.at[index, 'lwf'] = lwf\n",
        "    test_text.at[index, 'gf'] = gf\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZXtIPieYlYg"
      },
      "source": [
        "# Word Level Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhCp0gJMYlYh"
      },
      "source": [
        "# Method to get Measure of Textual Lexical Diversity\n",
        "def getMTLD(tokens):\n",
        "    types = []\n",
        "    factors=0\n",
        "    ttrThreshold = 0.72\n",
        "    startIndex = 0\n",
        "    ttr=1\n",
        "    # go over the text and get ttr to get the number of factors\n",
        "    for i in range(len(tokens)):\n",
        "        currentToken = tokens[i]\n",
        "        #each time a new type is found, compute type token ratio\n",
        "        if currentToken.lower() not in types:\n",
        "            types.append(currentToken.lower())\n",
        "        ttr = len(types)/(i+1-startIndex)\n",
        "        if ttr < ttrThreshold:\n",
        "       #cut text (those portions are called factor) and reset list of types\n",
        "            startIndex = i+1\n",
        "            types.clear()\n",
        "            #keep count of factors\n",
        "            factors +=1        \n",
        "        #if it is the last word and the ttr threshold is not reached, calculate the rest factor\n",
        "        elif (ttr >ttrThreshold) and (i ==len(tokens)-1):\n",
        "            factors += (1-ttr)/(1-0.72)\n",
        "    #repeat until all tokens are finished.\n",
        "    #form MTLD score:  #tokens /#factors\n",
        "    mtld1 = len(tokens) / factors\n",
        "    \n",
        "    #repeat same starting at the end of the text\n",
        "    factors = 0\n",
        "    startIndex= len(tokens)-1\n",
        "    ttr=1\n",
        "    types.clear()\n",
        "    for i in reversed(range(len(tokens))):\n",
        "        currentToken = tokens[i]\n",
        "        #each time a new type is found, compute type token ratio\n",
        "        if currentToken.lower() not in types:\n",
        "            types.append(currentToken.lower())\n",
        "        ttr = len(types)/(startIndex-i+1)\n",
        "\n",
        "        #when ttr reaches threshold\n",
        "        if ttr < ttrThreshold:\n",
        "            #cut text (those portions are called factor) and reset list of types \n",
        "            startIndex = i-1\n",
        "            types.clear()\n",
        "            factors +=1\n",
        "        #if it is the last word and the ttr threshold is not reached. calculate the rest factor\n",
        "        elif (ttr >ttrThreshold) and (i ==0):\n",
        "            factors += (1-ttr)/(1-0.72)\n",
        "            \n",
        "    #repeat until not tokens left\n",
        "\n",
        "    mtld2 = len(tokens)/ factors\n",
        "    res = (mtld1+mtld2)/2\n",
        "    resD = float(res)\n",
        "    #take the mean of both forward and backward score\n",
        "    if not np.isinf(resD):\n",
        "        return resD\n",
        "    else:\n",
        "        return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86EziuyYlYj"
      },
      "source": [
        "for index, row in test_text.iterrows():\n",
        "    essay_low = row['text'].lower()\n",
        "    essay_low_np = essay_low.replace('[^\\w\\s]','')\n",
        "    tokens_low_np = tokenizer.tokenize(essay_low_np)\n",
        "    types = set(tokens_low_np)\n",
        "    num_types = len(types)\n",
        "    num_tokens = len(tokens_low_np)\n",
        "    \n",
        "    test_text.at[index, 'Word_numWords'] = num_tokens\n",
        "    test_text.at[index, 'Word_TTR'] = handleDivByZero(num_types, num_tokens)\n",
        "    test_text.at[index, 'Word_CTTR'] = handleDivByZero(num_types, np.sqrt(2.0*num_tokens))\n",
        "    test_text.at[index, 'Word_RTTR'] = handleDivByZero(num_types,np.sqrt(num_tokens))\n",
        "    test_text.at[index, 'Word_BilogTTR'] = handleDivByZero(np.log(num_types),np.log(num_tokens))\n",
        "    test_text.at[index, 'Word_UberIndex'] = handleDivByZero(np.log(num_tokens)**2,np.log(num_tokens/num_types))\n",
        "    test_text.at[index, 'Word_MTLD'] = restrict2TwoDecimals(getMTLD(tokens_low_np))\n",
        "    \n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5mwOZJWYlYm"
      },
      "source": [
        "# Error features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NFeBcYhYlYo"
      },
      "source": [
        "# Used language_check library implementation for error detection\n",
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text']\n",
        "    sentences = sent_tokenize(essay)\n",
        "    num_sents = len(sentences)\n",
        "    spelling_mistake = 0\n",
        "    duplicate_mistake = 0\n",
        "    other_mistake = 0\n",
        "    for sentence in sentences:\n",
        "        matches = tool.check(sentence)\n",
        "        for match in matches:\n",
        "            if match.locqualityissuetype == 'misspelling':\n",
        "                spelling_mistake +=1\n",
        "            elif match.locqualityissuetype == 'duplication':\n",
        "                duplicate_mistake +=1\n",
        "            else:\n",
        "                other_mistake +=1\n",
        "                \n",
        "    all_mistakes = spelling_mistake + duplicate_mistake + other_mistake\n",
        "    \n",
        "    test_text.at[index, 'SpellingErrorsPerSen'] = restrict2TwoDecimals(spelling_mistake/num_sents)\n",
        "    test_text.at[index, 'DuplicateErrorsPerSen'] = restrict2TwoDecimals(duplicate_mistake/num_sents)\n",
        "    test_text.at[index, 'OtherErrorsPerSen'] = restrict2TwoDecimals(other_mistake/num_sents)\n",
        "    test_text.at[index, 'AllErrorsPerSen'] = restrict2TwoDecimals(all_mistakes/num_sents)\n",
        "    test_text.at[index, 'SpellingErrorsWrtAllErrors'] = restrict2TwoDecimals(spelling_mistake/all_mistakes)\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz8TL7IzYlYp"
      },
      "source": [
        "# Parts Of Speech (POS) Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j9c96bzYlYq"
      },
      "source": [
        "# Used NLTK library implementation of POS features\n",
        "# Run 'nltk.help.upenn_tagset()' to get list of The Penn Treebank's POS tags\n",
        "\n",
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text']\n",
        "    tokens = word_tokenize(essay)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    TotalWords = 0;\n",
        "    numAdj = 0;\n",
        "    numNouns = 0;\n",
        "    numVerbs = 0;\n",
        "    numPronouns = 0;\n",
        "    numConjunct = 0;\n",
        "    numProperNouns = 0;\t \n",
        "    numPrepositions = 0;\n",
        "    numAdverbs = 0;\n",
        "    numLexicals = 0;\n",
        "    numModals = 0;\n",
        "    numInterjections = 0;\n",
        "    perpronouns = 0;\n",
        "    whperpronouns = 0;\n",
        "    numauxverbs = 0;\n",
        "    numFunctionWords = 0;\n",
        "    numDeterminers = 0;\n",
        "    numVB = 0;\n",
        "    numVBD = 0;\n",
        "    numVBG = 0;\n",
        "    numVBN = 0;\n",
        "    numVBP = 0;\n",
        "    numVBZ = 0;\n",
        "    uniqueVerbs = []\n",
        "    for word, tag in tagged:\n",
        "        if tag ==\"PRP\" or tag ==\"PRP$\" or tag==\"WP\" or tag==\"WP$\":\n",
        "            numPronouns +=1\n",
        "            if tag == \"PRP\":\n",
        "                perpronouns +=1\n",
        "            if tag == \"WP\":\n",
        "                whperpronouns +=1\n",
        "            numFunctionWords +=1\n",
        "            TotalWords +=1\n",
        "            \n",
        "        if tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "            numVerbs+=1\n",
        "            TotalWords+=1\n",
        "            if word not in uniqueVerbs:\n",
        "                uniqueVerbs.append(word)\n",
        "            if tag == \"VB\":\n",
        "                numVB+=1\n",
        "            elif tag == \"VBD\":\n",
        "                numVBD+=1\n",
        "            elif tag == \"VBG\":\n",
        "                numVBG+=1\n",
        "            elif tag == \"VBN\":\n",
        "                numVBN+=1\n",
        "            if tag == \"VBP\":\n",
        "                numVBP+=1\n",
        "            if tag == \"VBZ\":\n",
        "                numVBZ+=1\n",
        "        if tag == \"JJ\" or tag == \"JJR\" or tag == \"JJS\":\n",
        "            numAdj+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"RB\" or tag == \"RBR\" or tag == \"RBS\" or tag == \"RP\":\n",
        "            numAdverbs+=1\n",
        "            numFunctionWords+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"IN\":\n",
        "            numPrepositions+=1\n",
        "            numFunctionWords+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"UH\":\n",
        "            numInterjections+=1\n",
        "            numFunctionWords+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"CC\":\n",
        "            numConjunct+=1\n",
        "            numFunctionWords+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"NN\" or tag == \"NNS\":\n",
        "            numNouns+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"NNP\" or tag == \"NNPS\":\n",
        "            numProperNouns+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"MD\":\n",
        "            numModals+=1\n",
        "            numauxverbs+=1\n",
        "            numFunctionWords+=1\n",
        "            TotalWords+=1\n",
        "        if tag == \"DT\":\n",
        "            numFunctionWords+=1\n",
        "            numDeterminers+=1\n",
        "            TotalWords+=1\n",
        "        #End of all words in a sentence.\n",
        "    #End of all sentences\n",
        "    numLexicals = numAdj+numNouns+numVerbs+numAdverbs+numProperNouns\n",
        "    numVerbsOnly = numVerbs-numauxverbs\n",
        "\n",
        "    test_text.at[index, 'POS_numNouns'] = restrict2TwoDecimals(numNouns+numProperNouns/TotalWords)\n",
        "    test_text.at[index, 'POS_numProperNouns'] = restrict2TwoDecimals(numProperNouns/TotalWords)\n",
        "    test_text.at[index, 'POS_numPronouns'] = restrict2TwoDecimals(numPronouns/TotalWords)\n",
        "    test_text.at[index, 'POS_numConjunct'] = restrict2TwoDecimals(numConjunct/TotalWords)\n",
        "    test_text.at[index, 'POS_numAdjectives'] = restrict2TwoDecimals(numAdj/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbs'] = restrict2TwoDecimals(numVerbs/TotalWords)\n",
        "    test_text.at[index, 'POS_numAdverbs'] = restrict2TwoDecimals(numAdverbs/TotalWords)\n",
        "    test_text.at[index, 'POS_numModals'] = restrict2TwoDecimals(numModals/TotalWords)\n",
        "    test_text.at[index, 'POS_numPrepositions'] = restrict2TwoDecimals(numPrepositions/TotalWords)\n",
        "    test_text.at[index, 'POS_numInterjections'] = restrict2TwoDecimals(numInterjections/TotalWords)\n",
        "    test_text.at[index, 'POS_numPerPronouns'] = restrict2TwoDecimals(perpronouns/TotalWords)\n",
        "    test_text.at[index, 'POS_numWhPronouns'] = restrict2TwoDecimals(whperpronouns/TotalWords)\n",
        "    test_text.at[index, 'POS_numLexicals'] = restrict2TwoDecimals((numLexicals)/TotalWords)\n",
        "    test_text.at[index, 'POS_numFunctionWords'] = restrict2TwoDecimals((numFunctionWords)/TotalWords)\n",
        "    test_text.at[index, 'POS_numDeterminers'] = restrict2TwoDecimals((numDeterminers)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVB'] = restrict2TwoDecimals((numVB)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVBD'] = restrict2TwoDecimals((numVBD)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVBG'] = restrict2TwoDecimals((numVBG)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVBN'] = restrict2TwoDecimals((numVBN)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVBP'] = restrict2TwoDecimals((numVBP)/TotalWords)\n",
        "    test_text.at[index, 'POS_numVerbsVBZ'] = restrict2TwoDecimals((numVBZ)/TotalWords)\n",
        "    test_text.at[index, 'POS_advVar'] = restrict2TwoDecimals(numAdverbs/numLexicals)\n",
        "    test_text.at[index, 'POS_adjVar'] = restrict2TwoDecimals(numAdj/numLexicals)\n",
        "    test_text.at[index, 'POS_modVar'] = restrict2TwoDecimals((numAdj+numAdverbs)/numLexicals)\n",
        "    test_text.at[index, 'POS_nounVar'] = restrict2TwoDecimals((numNouns+numProperNouns)/numLexicals)\n",
        "    test_text.at[index, 'POS_verbVar1'] = restrict2TwoDecimals((numVerbsOnly)/len(uniqueVerbs))\n",
        "    test_text.at[index, 'POS_verbVar2'] = restrict2TwoDecimals((numVerbsOnly)/numLexicals)\n",
        "    test_text.at[index, 'POS_squaredVerbVar1'] = restrict2TwoDecimals((numVerbsOnly*numVerbsOnly)/len(uniqueVerbs))\n",
        "    test_text.at[index, 'POS_correctedVV1'] = restrict2TwoDecimals((numVerbsOnly)/np.sqrt(2.0*len(uniqueVerbs)))\n",
        "    \n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWNWMM_CYlYt"
      },
      "source": [
        "# Syntactic Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSeQPbUqYlYx"
      },
      "source": [
        "# This code generates parse tree for the given text and make syntactic calculations.\n",
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text']\n",
        "    sentences = sent_tokenize(essay)\n",
        "    numSBAR = 0\n",
        "    avgParseTreeHeight = 0\n",
        "    numNP = 0\n",
        "    numVP = 0\n",
        "    numPP = 0\n",
        "    numSubtrees = 0\n",
        "    numWhPhrases = 0\n",
        "    numConjPhrases = 0\n",
        "    reducedRelClauses = 0\n",
        "    numWords = 0\n",
        "    numClauses = 0\n",
        "    numTunits = 0\n",
        "    numComplexNominals = 0\n",
        "    numDependentClauses = 0\n",
        "    numCoordinateClauses = 0\n",
        "    numComplexTunits = 0\n",
        "    AvgNPSize = 0\n",
        "    AvgVPSize = 0\n",
        "    AvgPPSize = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        try:\n",
        "            cn_tree = nlp.parse(sentence)\n",
        "            tree = Tree.fromstring(cn_tree)\n",
        "            avgParseTreeHeight += tree.height()\n",
        "            numWords += len(tree.leaves())\n",
        "            for st in tree.subtrees():\n",
        "                numSubtrees +=1\n",
        "                if st.label() == \"NP\":\n",
        "                    numNP += 1\n",
        "                    AvgNPSize += len(st)\n",
        "\n",
        "                if st.label() == \"VP\":\n",
        "                    numVP += 1\n",
        "                    AvgVPSize += len(st)\n",
        "\n",
        "                if st.label() == \"PP\":\n",
        "                    numPP += 1\n",
        "                    AvgPPSize += len(st)\n",
        "\n",
        "                if st.label() == \"WHNP\" or st.label() == \"WHPP\" or st.label() == \"WHADVP\" or st.label() == \"WHADJP\":\n",
        "                    numWhPhrases += 1\n",
        "\n",
        "                if st.label() == \"RRC\":\n",
        "                    reducedRelClauses += 1\n",
        "\n",
        "                if st.label() == \"CONJP\":\n",
        "                    numConjPhrases += 1\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "    numSentences = len(sentences)\n",
        "    \n",
        "    test_text.at[index, 'SYN_numSentences'] = numSentences\n",
        "    test_text.at[index, 'SYN_avgSentenceLength'] = restrict2TwoDecimals(numWords/numSentences)\n",
        "    test_text.at[index, 'SYN_avgParseTreeHeightPerSen'] = restrict2TwoDecimals(avgParseTreeHeight/numSentences)\n",
        "    test_text.at[index, 'SYN_numSubtreesPerSen'] = restrict2TwoDecimals(numSubtrees/numSentences)\n",
        "    test_text.at[index, 'SYN_numNPsPerSen'] = restrict2TwoDecimals(numNP/numSentences)\n",
        "    test_text.at[index, 'SYN_numVPsPerSen'] = restrict2TwoDecimals(numVP/numSentences)\n",
        "    test_text.at[index, 'SYN_numPPsPerSen'] = restrict2TwoDecimals(numPP/numSentences)\n",
        "    test_text.at[index, 'SYN_numNPSize'] = handleDivByZero(AvgNPSize,numNP)\n",
        "    test_text.at[index, 'SYN_numVPSize'] = handleDivByZero(AvgVPSize,numVP)\n",
        "    test_text.at[index, 'SYN_numPPSize'] = handleDivByZero(AvgPPSize,numPP)\n",
        "    test_text.at[index, 'SYN_numWHPsPerSen'] = restrict2TwoDecimals(numWhPhrases/numSentences)\n",
        "    test_text.at[index, 'SYN_numRRCsPerSen'] = restrict2TwoDecimals(reducedRelClauses/numSentences)\n",
        "    test_text.at[index, 'SYN_numConjPPerSen'] = restrict2TwoDecimals(numConjPhrases/numSentences)\n",
        "    \n",
        "test_text   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VWvzHRtaw5j"
      },
      "source": [
        "# Discourse Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34-xql5wYlYy"
      },
      "source": [
        "### Referring Expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHO39QrjYlYy"
      },
      "source": [
        "# This code generates discourse features related to referring expressions\n",
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text']\n",
        "    sentences = sent_tokenize(essay)\n",
        "    tokens = word_tokenize(essay)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    numWords = 0.0\n",
        "    numSentences = len(sentences)\n",
        "    numPronouns = 0.0\n",
        "    numPersonalPronouns = 0.0\n",
        "    numPossessivePronouns = 0.0\n",
        "    numDefiniteArticles = 0.0\n",
        "    numProperNouns = 0.0\n",
        "    numNouns = 0.0\n",
        "    for word, tag in tagged:\n",
        "        if word[0].isalpha():\n",
        "            numWords +=1\n",
        "            if tag == 'DT' and word.lower() == 'the':\n",
        "                numDefiniteArticles +=1\n",
        "            elif tag == 'PRP':\n",
        "                numPersonalPronouns +=1\n",
        "            elif tag == 'PRP$':\n",
        "                numPossessivePronouns +=1\n",
        "            elif tag.startswith('NN'):\n",
        "                numNouns +=1\n",
        "                if tag.startswith('NNP'):\n",
        "                    numProperNouns +=1\n",
        "                    \n",
        "    numPronouns = numPersonalPronouns + numPossessivePronouns\n",
        "    \n",
        "    test_text.at[index, 'DISC_RefExprPronounsPerNoun'] = restrict2TwoDecimals(numPronouns/numNouns)\n",
        "    test_text.at[index, 'DISC_RefExprPronounsPerSen'] = restrict2TwoDecimals(numPronouns/numSentences)\n",
        "    test_text.at[index, 'DISC_RefExprPronounsPerWord'] = restrict2TwoDecimals(numPronouns/numWords)\n",
        "    test_text.at[index, 'DISC_RefExprPerPronounsPerSen'] = restrict2TwoDecimals(numPersonalPronouns/numSentences)\n",
        "    test_text.at[index, 'DISC_RefExprPerProPerWord'] = restrict2TwoDecimals(numPersonalPronouns/numWords)\n",
        "    test_text.at[index, 'DISC_RefExprPossProPerSen'] = restrict2TwoDecimals(numPossessivePronouns/numSentences)\n",
        "    test_text.at[index, 'DISC_RefExprPossProPerWord'] = restrict2TwoDecimals(numPossessivePronouns/numWords)\n",
        "    test_text.at[index, 'DISC_RefExprDefArtPerSen'] = restrict2TwoDecimals(numDefiniteArticles/numSentences)\n",
        "    test_text.at[index, 'DISC_RefExprDefArtPerWord'] = restrict2TwoDecimals(numDefiniteArticles/numWords)\n",
        "    test_text.at[index, 'DISC_RefExprProperNounsPerNoun'] = restrict2TwoDecimals(numProperNouns/numNouns)\n",
        "\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5buj0ocZYlYz"
      },
      "source": [
        "### Content Overlap features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDmyGL9IYlY0"
      },
      "source": [
        "def getGeneralTag(specificTag):\n",
        "    generaltag = \"NOTAG\"\n",
        "    if specificTag.startswith(\"VB\"): \n",
        "        generaltag = \"VERB\"\n",
        "\n",
        "    elif specificTag.startswith(\"JJ\"):\n",
        "        generaltag = \"ADJECTIVE\"\n",
        "\n",
        "    elif specificTag.startswith(\"RB\") or specificTag == \"WRB\" :\n",
        "        generaltag = \"ADVERB\"\n",
        "        \n",
        "    elif specificTag.startswith(\"PRP\") or specificTag.startswith(\"WP\"):\n",
        "        generaltag = \"PRONOUN\"\n",
        "\n",
        "    elif specificTag.startswith(\"NN\"):\n",
        "        generaltag = \"NOUN\"\n",
        "\n",
        "    return generaltag\n",
        "\n",
        "def lemma_function(word, tag):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return lemmatizer.lemmatize(word)\n",
        "    \n",
        "def returnFormattedSentence(sentences):\n",
        "    formated_sent = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        single_sent = []\n",
        "        for word, tag in tagged:\n",
        "            word_prop = []\n",
        "            word_prop.append(word.lower())\n",
        "            word_prop.append(lemma_function(word, tag))\n",
        "            word_prop.append(tag)\n",
        "            word_prop.append(getGeneralTag(tag))\n",
        "            single_sent.append(word_prop)\n",
        "        formated_sent.append(single_sent)\n",
        "    return formated_sent\n",
        "\n",
        "def isThereNounOverlap(sent_1, sent_2):\n",
        "    for word in sent_1:\n",
        "        if (word[3]==\"NOUN\") and (word in sent_2):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def isThereArgumentOverlap(sent_1, sent_2):\n",
        "    if isThereNounOverlap(sent_1, sent_2):\n",
        "        return True\n",
        "    else:\n",
        "        for word in sent_1:\n",
        "            if (word[3]==\"PRONOUN\") and (word in sent_2):\n",
        "                return True\n",
        "            else:\n",
        "                if word[3]==\"NOUN\" or word[3]==\"PRONOUN\":\n",
        "                    word_lemma_1 = word[1]\n",
        "                    word_pos_1 = word[3]\n",
        "                    for word2 in sent_2:\n",
        "                        word_lemma_2 = word2[1]\n",
        "                        word_pos_2 = word2[3]\n",
        "                        if (word_lemma_1 == word_lemma_2) and (word_pos_1 == word_pos_2) and (not word_pos_1 == \"NOTAG\"):\n",
        "                            return True\n",
        "    return False\n",
        "\n",
        "def isThereStemOverlap(sent_1, sent_2):\n",
        "    if isThereNounOverlap(sent_1, sent_2) or isThereArgumentOverlap(sent_1, sent_2):\n",
        "        return True\n",
        "    else:\n",
        "        for word in sent_1:\n",
        "            if not word[3] == \"NOTAG\":\n",
        "                word_lemma_1 = word[1]\n",
        "                word_pos_1 = word[3]\n",
        "                for word2 in sent_2:\n",
        "                    word_lemma_2 = word2[1]\n",
        "                    word_pos_2 = word2[3]\n",
        "                    if (word_lemma_1 == word_lemma_2) and (word_pos_1==\"NOUN\" or word_pos_2==\"NOUN\" or word_pos_1 == \"PRONOUN\"):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "def contentWordOverlap(sent_1, sent_2):\n",
        "    overlapsCount = 0\n",
        "    for word in sent_1:\n",
        "        word_lemma_1 = word[1]\n",
        "        word_pos_1 = word[3]\n",
        "        if (not word_pos_1 == \"NOTAG\") and (not word_pos_1 == \"PRONOUN\"):\n",
        "            for word2 in sent_2:\n",
        "                if word_lemma_1 == word2[1]:\n",
        "                    overlapsCount +=1\n",
        "    return overlapsCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP-WKRynYlY1"
      },
      "source": [
        "for index, row in test_text.iterrows():\n",
        "    essay = row['text']\n",
        "    sentences = sent_tokenize(essay)\n",
        "    formatted_sentences = returnFormattedSentence(sentences)\n",
        "    localNounOverlapCount = 0\n",
        "    localArgumentOverlapCount = 0\n",
        "    localStemOverlapCount = 0\n",
        "    localContentWordOverlap = 0\n",
        "\n",
        "    globalNounOverlapCount = 0\n",
        "    globalArgumentOverlapCount = 0\n",
        "    globalStemOverlapCount = 0\n",
        "    globalContentWordOverlap = 0\n",
        "    \n",
        "    totalSentencesSize = len(sentences)\n",
        "    for i in range(0,totalSentencesSize):\n",
        "        for j in range(i+1,totalSentencesSize):\n",
        "            sent_1, sent_2 = formatted_sentences[i], formatted_sentences[j]\n",
        "            if isThereNounOverlap(sent_1, sent_2):\n",
        "                if (j-i) == 1:\n",
        "                    localNounOverlapCount +=1\n",
        "                    localArgumentOverlapCount +=1\n",
        "                    localStemOverlapCount +=1\n",
        "                globalNounOverlapCount +=1\n",
        "                globalArgumentOverlapCount +=1\n",
        "                globalStemOverlapCount +=1\n",
        "            elif isThereArgumentOverlap(sent_1, sent_2):\n",
        "                if (j-i) ==1:\n",
        "                    localArgumentOverlapCount +=1\n",
        "                    localStemOverlapCount +=1\n",
        "                globalArgumentOverlapCount +=1\n",
        "                globalStemOverlapCount +=1\n",
        "            elif isThereStemOverlap(sent_1, sent_2):\n",
        "                if (j-i) ==1:\n",
        "                    localStemOverlapCount +=1\n",
        "                globalStemOverlapCount +=1\n",
        "            tempContentOverlap = contentWordOverlap(sent_1, sent_2)\n",
        "            globalContentWordOverlap += tempContentOverlap\n",
        "            if (j-i) ==1:\n",
        "                localContentWordOverlap += tempContentOverlap\n",
        "    test_text.at[index, 'total_sentences'] = totalSentencesSize\n",
        "    test_text.at[index, 'DISC_localNounOverlapCount'] = restrict2TwoDecimals(localNounOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_localArgumentOverlapCount'] = restrict2TwoDecimals(localArgumentOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_localStemOverlapCount'] = restrict2TwoDecimals(localStemOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_localContentWordOverlapCount'] = restrict2TwoDecimals(localContentWordOverlap/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_globalNounOverlapCount'] = restrict2TwoDecimals(globalNounOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_globalArgumentOverlapCount'] = restrict2TwoDecimals(globalArgumentOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_globalStemOverlapCount'] = restrict2TwoDecimals(globalStemOverlapCount/totalSentencesSize)\n",
        "    test_text.at[index, 'DISC_globalContentWordOverlapCount'] = restrict2TwoDecimals(globalContentWordOverlap/totalSentencesSize)\n",
        "\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LLn1CPjZYlY2"
      },
      "source": [
        "# path to Discourse output text marked with connective tags.\n",
        "path = 'C:\\\\Users\\\\hp word\\\\Documents\\\\University of Bath Labs\\\\SEM 2\\\\Research project dissertation\\\\Codes_Libraries\\\\My_code\\\\Feature_engineering\\\\DiscourseFeaturesData\\\\TestDiscOutputTxt'\n",
        "\n",
        "# r=root, d=directories, f = files\n",
        "for r, d, f in os.walk(path):\n",
        "    for file in f:\n",
        "        with open (str(os.path.join(r, file)), \"r\") as myfile:\n",
        "            data=myfile.readlines()\n",
        "            \n",
        "            numNonDiscConnectives = len([k for k in data if '#0' in k])\n",
        "            numCompConnectives = len([k for k in data if '#Comparison' in k])\n",
        "            numExpConnectives = len([k for k in data if '#Expansion' in k])\n",
        "            numContConnectives = len([k for k in data if '#Contingency' in k])\n",
        "            numTempConnectives = len([k for k in data if '#Temporal' in k])\n",
        "            \n",
        "            numDiscConnectives = numCompConnectives + numExpConnectives + numContConnectives + numTempConnectives;\n",
        "            numConnectives = numDiscConnectives + numNonDiscConnectives;\n",
        "        \n",
        "            numSentences = test_text.loc[test_text['doc_id'] == str(file)]['total_sentences'].values[0]\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_00_numConnectivesPerSen'] = restrict2TwoDecimals(numConnectives/numSentences)\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_01_numDiscConnectivesPerSen'] = restrict2TwoDecimals(numDiscConnectives/numSentences)\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_02_numNonDiscConnectivesPerSen'] = restrict2TwoDecimals(numNonDiscConnectives/numSentences)\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_03_numCompConnectivesPerSen'] = restrict2TwoDecimals(numCompConnectives/numSentences)\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_04_numExpConnectivesPerSen'] = restrict2TwoDecimals(numExpConnectives/numSentences)\n",
        "            test_text.loc[test_text['doc_id'] == str(file),'DISCPlus_05_numContConnectives'] = restrict2TwoDecimals(numContConnectives/numSentences)\n",
        "            myfile.close()\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTwkE40a2krX"
      },
      "source": [
        "# Creating a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr8Vg3XV2y5U"
      },
      "source": [
        "# Removing a duplicate feature\n",
        "test_text = test_text.drop(\"total_sentences\", axis = 1)\n",
        "test_text.to_csv(r'test_features - Copy.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}